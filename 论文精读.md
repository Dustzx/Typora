## 论文精读

方法论：

三遍：

1. 标题+摘要+结论+实验部分图表 最终决定是否继续读
2. 重要图表的详细内容+圈出引用文献 
3. 复现作者的思路，并有自己的想法

①标题+作者

②摘要

③结论

④导言

⑤相关工作

⑥模型

⑦实验

⑧评论

### 1. Transformer 

②主流的序列转录模型（由所给序列生成目标序列）大多都基于复杂的循环或卷积神经网络，都有一个编码器和解码器。其中表现最佳的模型也会在编码器和解码器之间使用到注意力机制。基于注意力机制作者提出了一个新的简单的神经网络架构，**Transformer**，该模型仅仅基于注意力机制。

③Transformer 是第一个仅仅使用注意力机制的转录模型，它将之前的在编码解码器之间使用的循环层替换为了multi-head self-attention

在机器翻译这一任务上，Transformer训练地比其他传统的架构都要快。

④RNN对于一个序列的计算是从左往右一步一步做，对于第t个词会计算隐藏状态ht，该ht由前一个词的ht-1和当前词一起决定。该时序性的计算使得并行难以进行。

​	并且Attention机制早已应用于编码器与解码器的结合部，用来使编码器的东西很有效地传给解码器。

​	Transformer不再使用之前的循环神经层，而是仅使用注意力机制去描绘输入和输出之间的全局依赖关系。它支持更强的并行，并且可以在更短时间内完成更为高质量的任务。

⑤  Extended Neural 、GPU ByteNet 、ConvS2S都通过使用卷积神经网络为基本单位进行构建，并行计算所有输入输出位置的隐藏表示，从而减少顺序计算增加并发度。对于这些模型，将来自两个任意输入或输出位置的信号关联起来所需的操作数量随着位置之间的距离而增长，对于ConvS2S来说是线性增长，对于ByteNet来说是对数增长。

​	而在Transformer这些运算的数量被减少到了常量级别，以此为代价的是由于注意力权重位置的平均化导致的辨识度的降低，对于这一缺点，采用Multi-Head Attention机制来解决。

​	Self-attention,或者称为intra-attention，是将一个序列中不同位置关联起来的注意力机制，以计算序列的表示。

​	Transformer是第一个只使用自注意力机制来做encode、decode架构的模型

⑥ 大多数有竞争力的神经网络序列转录模型都有一个encoder-decoder架构。encoder将输入序列的符号表示x(x1,……xn)转换成一个连续的向量表示z(z1,……zn)。对于z decoder将一次解码出一个y最终生成序列y(y1,……yn)，每次生成都是一次auto-regressive自回归，对于yt则需要y1~yt-1作为输入。

 	Transformer也使用了encoder-decoder架构，具体来说该encoder-decoder使用了堆叠起来的self-attention 、point-wise和全连接层

​	编码器结构如下<img src="NLP学习记录.assets/1665126121911.png" alt="1665126121911" style="zoom: 67%;" />

输入先进入嵌入层，将词转换为向量，随后连接的是N层的由Muti-Head Attention以及Feed Forward(前馈神经网络)构成的块，【Add&Norm】中连接到Add的为

**残差连接**

(将浅层输出与深层输出求和 we hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping . To the extreme,  if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers 残差块使得训练很深的网络更加容易)，Norm为LayerNormalization

<img src="NLP学习记录.assets/1665126138890.png" alt="1665126138890" style="zoom:50%;" />

残差连接可以解决**梯度消失**的问题(防止梯度<1相乘后无限接近于0)，残差连接后使得梯度保持在1左右

在使用残差连接之前，更深的神经网络并不能比浅层神经网络具有更好的效果(更深的网络，误差率(训练+测试)反而更高)。

实际上浅层网络构建好后，后加的网络充当一个identity mapping 的话，深层网络的精确度不应该降低，但实际来说SGD并无法实现这一点。

残差连接则是将从前一层传递过来的$H(X)$不直接去学习，而是学习$H(X)-x$, 并在输出时加上那个减去的$x$.

即

《Identity Mappings in Deep Residual Networks》中介绍了各种residual块的设计。

​	LayerNorm与batchNorm比较(蓝色为batchNorm)，layerNorm是对一个样本所有特征进行计算，BatchNorm是对一个mini-batch中的一个特征进行计算

当输入为2D<img src="NLP学习记录.assets/1665126147831.png" alt="1665126147831" style="zoom:50%;" />

二者通过对数据的转置可以达到统一的效果

而RNN、Transformer中输入为3D，如图

<img src="NLP学习记录.assets/1665126156370.png" alt="1665126156370" style="zoom:50%;" />

由于LayerNorm、BatchNorm两种切法不同以及每个序列长度的不固定性，导致了BatchNorm在每次小批量计算时的均值方差的抖动相对较大 ，同时也导致其全局的均值方差不准确（可能新的序列长度过长或过短）；而LayerNorm小批量计算的是每个样本自己的均值和方差，并且也没有必要存储全局均值方差（测试时），故相对稳定。

<img src="NLP学习记录.assets/1665125059900.png" alt="1665125059900" style="zoom:50%;" />

解码器结构如下

<img src="NLP学习记录.assets/1665126164383.png" alt="1665126164383" style="zoom: 67%;" />

解码器的自回归机制(t-1时刻的输出作为t时刻的输入)，以及attention机制中能看到完整的输入，故需要带掩码的注意力机制即Masked Attention，来保证在t时间的输入不会看到t时间之后的内容

​	**Attention机制**就是将query查询内容根据键值对key-value中与key的相似度映射为一个output，其中key-value保持不变，随着query权重分配的变化，将会有不同的output。在计算相似度时，不同的Attention版本有不同的算法。(涉及到的所有数据都是向量)

​	Transformer在计算注意力时使用的是sclaed dot-product attention。该方法中query和key的维度相等，通过计算两个向量的内积来衡量其相似度，内积越大则相似度越高(？)（long相等的前提下）,**Attention(Q,K,V)=softmax(Q K内积/向量长度) V**。除以向量长度是防止两个向量长度比较长时，出现较大值的概率将会增加，该相对差距变大的可能性增加后使得softmax后该值更加靠近于1，剩余的值则更加靠近于0，在该种情况下softmax回归计算时梯度将会很小，不利于尽快收敛。而Transformer中的向量长度都是比较大的故应除以√dk。 计算流程图如下

<img src="NLP学习记录.assets/1665126171051.png" alt="1665126171051" style="zoom:67%;" />

##### Muti-Head Attention

<img src="NLP学习记录.assets/1665126183494.png" alt="1665126183494" style="zoom:67%;" />

相较于单个的注意力函数直接去计算高维的向量，将其投影到低维度并行地去计算更有好处 ，如上图将V、K、Q分别进行投影，投影h次，而每次投影时的W是一直在学习的。
$$
MultiHead(Q,K,V) = Concat(head_1,……,head_h)W^o
$$

$$
head_i= Attention(QW_i^Q,KW_i^K,VW_i^V)
$$

##### Position-wise Feed-Forward Networks

实际上是一个全连接的前馈神经网络，用来作用于每一个词(position)
$$
FFN(x) = max(0, xW1 + b1)W2 + b2
$$
W1将d=512的x扩大到d=2048，线性相加后Relu，然后用W2将维度降回512，最后再线性相加

##### Positional Encoding

用与embedding后数据位数等长的数据来表示该数据原始的位置信息，相加后即携带了该词的位置信息 		

⑦编码器和解码器的embedding 由于使用了统一的字典所以共享权重

⑧**评价**：Attention并不是ALL you need，其中的前馈神经网络、残差连接都缺一不可 。

### 2. Bert

②Abstract

Bidirectional Encoder Representations from Transformers

Bert全称为transformer模型的双向编码器表示

bert使得NLP的语言模型预训练正式出圈，它与最近的语言表示模型不同，bert通过联合所有层中左右的上下文信息，使用无标签的数据来训练深层双向的表示。预训练的bert模型只需要一个额外的输出层就能得到一个不错的结果。

（论文成果在摘要中写明基于什么工作，并且相对于该工作有何提升，再给出具体的实验数据，绝对精度+与当前最优相比提升的精度）

It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MutiNLI accuracy to 86.7% (4.6% absolute improvement) and SQuAD v2.0 Test F1 to 83.1(5.1 point absolute improvment)

已经存在的预训练模型分为基于特征的、基于微调的。

ELMo则属于基于特征的，每个下游任务都要构造一个与其相关的神经网络(RNN架构)，将预训练好的表示作为一个额外的特征同输入一起放入模型，使得模型训练起来比较容易。

GPT则是基于微调的，预训练好的参数在下游只需要微调

以上两个方案在预训练时都使用相同的目标函数，并且都使用单向的语言模型（为预测模型，预测下一个时刻所要输出的语言，故为单向）。

Bert则可用''带掩码的语言模型''(Masked language model, MLM)来减轻语言模型单向的限制，其灵感来自**Cloze task (Taylor，1953)**，具体来说，每次随机从输入中选择一些tokens并将其掩盖，目标函数则去预测这些被盖住的词（相当于进行完形填空），MLM允许去看左右两边的信息，这就使得我们可以训练出双向的深的Transformer

Bert是第一个在句子层面和词元层面取得好成绩的微调模型

③conclusion

最近一些实验表明，大量的、非监督的预训练对于很多语言模型来说是非常好的，这使得一些即使训练样本比较少的任务可以享受深度神经网络。bert的主要成果就是将已有成果拓展到了深的双向的架构上来，使得同样的预训练模型可以处理大量的不一样的NLP任务

⑤相关工作

非监督的基于特征的工作(ELMO)

非监督的基于微调的工作(GPT)

有标号的数据上做迁移学习

⑥BERT

该框架有两个步骤：1）预训练 2）微调

预训练时模型是在没有标号的数据集上训练的。

在微调时bert模型的权重被初始化为预训练时得到的权重，所有权重在微调时都会参与训练，并且使用的是下游任务的有标号的数据。

每一个下游任务都会单独建立一个模型并进行微调。

bert使用的架构是多层双向的Transformer编码器，该架构基于Transformer原始代码。

输入输出的表示上，输入统一为一个序列，从而无差别地表示一个句子或多个句子。这使得一个句子可以是连续文本中的任意跨度，而不是一个真实语义上的句子。

bert使用WordPiece去切词，每个序列的第一个词永远是[CLS(classificiation)]。句子之间用[SEP]特殊标记来分割。并且可以通过学习到的嵌入层来区分token属于A句子还是B句子

 <img src="NLP学习记录.assets/1665126193703.png" alt="1665126193703" style="zoom: 67%;" />

input经过三层embedding后求和，分别是词元本身的向量，所在句子信息向量和整体的position向量(可学的)，如下图所示。

<img src="NLP学习记录.assets/1665126208275.png" alt="1665126208275" style="zoom:50%;" />

以上为预训练和微调的相同部分。

预训练任务1：

在预训练时对于每个序列的wordpiece的词元随机选取了15%来进行替换，但由于微调时不存在[MASK]符号，这将导致预训练和微调时数据的不匹配。为了缓和这一问题，将15%被选中的词元中80%的用[MASK]替代，10%的随机替换一个词元，剩余10%不进行操作。以上三种情况都会被标记为用来做预测。

预训练任务2：

预测一个句子对中两个句子是不是相邻

在训练样本中：

50%概率选择相邻句子对：<cls>this movie is great <sep> i like it <sep>

50%概率选择随机句子对：<cls>this movie is great <sep> hello world <sep>

将<cls>对应的输出放到一个全连接层来预测

微调时由于句子对放到了一个Transformer块中，所以self-attention可以来回看，比起encoder-decoder架构更优，由此付出的代价是无法再做机器翻译了。

微调

单文本分类(如情感分析和测试句子语法可接受性）

<img src="论文精读.assets/1667232047609.png" alt="1667232047609" style="zoom: 67%;" />

文本对分类或回归(如自然语言推断和语义文本相似性)

<img src="论文精读.assets/1667232191946.png" alt="1667232191946" style="zoom:67%;" />

文本标注（词元级别的任务如词性标注）

<img src="论文精读.assets/1667232244055.png" alt="1667232244055" style="zoom:67%;" />

问答

![1667232333468](论文精读.assets/1667232333468.png)

 在给定问题和段落的情况下预测段落中文本片段的开始和结束。 

### 3. ResNet

②摘要

提出了一个可以简化深层神经网络训练复杂程度的残差学习块，并通过大量实验证明了，残差网络更容易去优化，并且在相当深的网络中仍然能提高精度。ResNet在ImageNet数据集上，使用比VGG Net深8倍的层数，即152层，仍然有更低的复杂度。这些层ResNets在ImageNet测试集上的错误率仅为3.57%。这一结果在ILSVRC 2015分类任务上排在首位。本文也完成了在CIFAR-10数据集上用100和1000层的结果分析。

③结论

CVPR要求正文不能超过8页，故由于本文实验结果太多，最终没有结论

本文优势：We can fairly compare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computational cost(except for the negligible element-wise addition).

④导言

不收敛->精度不高->ResNet提升精度

显然不同数据集上结果较好的，都是使用较为深的神经网络。但好的神经网络却不是简单地去堆叠层数就有效果的。这就是由于梯度消失、梯度爆炸的存在，这一问题会阻碍模型数据的收敛，但也被初始化时的归一化操作和中间的归一化层较好地解决了。收敛问题解决后，仍然存在着深层神经网络精度不高的问题，这一相较于浅层神经网络而言精度下降的问题，并不是由于模型的过拟合导致的。

解决这一问题的方法是，将新添加的层采用identity mapping（恒等映射），其他层相较原来的浅层网络模型不变。

本文则是采用了另一个方法解决degradation问题，一个深的残差学习架构。将下层的映射定义为$H (X)$，对于叠加的非线性层采用的映射为$F(X)=H(X)-X$, 原本的映射就成了$F(X)+X$. 该映射可以被认为是一个带短接的前馈神经网络。Shortcut connections(短接)是指跳跃一层甚至多层的一个恒等映射，该短接会与残差块的最后一层输出相加，如图所示

<img src="论文精读.assets/1665407343887.png" alt="1665407343887" style="zoom: 67%;" />

这一短接操作，既没有增加额外的参数，也没有增加计算的复杂度。整个网络也仍能使用随机梯度下降算法通过反向传播来进行端到端训练。

在使用ImageNet为数据集进行的对比实验中发现，1）使用残差连接块的深层网络容易去优化，而对应的不使用残差连接块的网络训练错误率会随着网络的深度增加而变高。2）残差网络的精确率会随网络深度的增加而提高。这一实验效果也同样出现在CIFAR-10数据集上。具体来说，在ImageNet 测试集上的Top-5 error达到了3.57%，并且在ILSVRC 2015分类竞赛上赢得了第一名。不仅如此，这一深度学习模型还有很好的泛化性，在ImageNet detection、ImageNet localization、COCO detection、COCO segmentation都拿到了2015年竞赛的第一名，这说明了残差连接的普适性。

⑤相关工作

有做过Residual Representation的：

VLAD、Fisher Vector(a probabilistic version of VLAD)

与残差连接公式出现的: "highway networks"，该网络的短接操作是连接门控的，该门控可以对短接通过参数进行调整，而不像残差连接中的短接，是参数无关的。该方法的弊端是，当门控参数趋近于0时，短接的效果将会消失，这与不进行短接并没有区别。并且‘’highway networks‘’文章中并没有表明深度超过100层时精确度是否有提升(多少有点杠，highway文章中最深100 layers)

⑥模型

<img src="论文精读.assets/1665463403175.png" alt="1665463403175" style="zoom:50%;" />

该残差块等价于公式
$$
y = F(x, \{W_i\}) + x.
$$


 其中$F(x, {W_i})$表示的是需要学习的残差映射，对应上图为$F=W_2σ(W_1x)+b$，σ此处使用的是relu函数，shortcut connection短接操作即实现了$F(x)+x$,并且在进行短接之后，又进行了一次relu操作。

公式中F和x的维度必须是相等的，当由于进行维度变化导致维度不一致时，可以引进一个$W_s$，将公式改为：
$$
y=F(x,\{W_i\})+W_sx.
$$



<img src="论文精读.assets/1665492921126.png" alt="1665492921126" style="zoom:67%;" />

残差块的大小(即中间跨越的层数)是可以变化的，即可以跨越多个层，本文使用的是2~3层。但是连接只跨越一层时公式(4)的效果就等价为了$y=W_1x+x$，相当于残差退化为了一个bias，故对模型的精准度没有任何帮助。

另一个重要的点是，残差对于不同类型的中间层都有着优化效果，比如中间层是MLP、CNN都可以

公式(4)当且仅当输入输出维度相同时才可以使用，当维度不同时，本文给出了两种方法A：短接参数不变，对于增加的维度对应补0，该方法没有引入额外参数 B：使用公式(5)去进行维度匹配

在具体应用时，在每个卷积层后激活函数前，都进行了Batch Normalization操作，使用了批量大小为256的mini-batch通过随机梯度下降进行训练。并将学习率从0.1开始，每当错误率趋于平稳时进行除10操作。使用了weight decay 参数为0.0001并且将momentum设置为0.9，但并没有用dropout(因为没有全连接层

⑦实验	

图片分类实验部分，使用了ImageNet 2012 classification数据集，该数据集中包含1000种图片类别。训练集中包含128万张图片，验证集中有5万张。最终评估结果的测试集中为10万张，并且计算了top-1和top-5 error rates

为了对比，构建了两个18层和两个34层的模型，分别称为plain和ResNet，区别为ResNet添加了残差连接

![1665500059006](论文精读.assets/1665500059006.png)

实验结果显示：添加了残差连接的ResNet随着神经网络深度的增加，精确度仍能提升，而plain则会随着深度增加精准度下降。并且同为18层时，二者的效果区别不大。

![1665500334790](论文精读.assets/1665500334790.png)

但是，同为18层的ResNet的收敛速度相比plain要快(对比前期第一次骤降之前)。

实验还对比了对于三种不同的残差连接公式选择的影响，A：对于维度增多的采用补0升维度的策略 B：对于需要升维度的使用公式(5)即做投影，其他所有短接采用(4)即恒等映射 C：所有短接都用公式(5).

最终结果是C>B>A，但是他们之间的差别是细微的如下图

<img src="论文精读.assets/1665501290868.png" alt="1665501290868" style="zoom:67%;" />

由于使用C会增加模型的复杂度，故作者并未使用C，而是使用B来完成后续实验(B只会做几次维度改变，故复杂度增加不大)

⑧评论

question: 

1.$F(X)=H(X)-X$如何实现

2.BatchNormalization在relu之前和之后有什么区别

3.为什么34层residual复杂度比19层VGG低

4.为什么短接方案C相较B会更精确，即C中有什么因素导致精度变高

5.深度增加在不使用ResNet的情况下为什么越深越不好

Thinking:

1.残差连接进行短接时F(X)+X中X传递添加可学习的系数，并且该短接默认存在于任意两层之间。（即是否存在有一些时刻，进行短接后效果反而不好，而添加系数，并对关键节点系数波动范围加以限制，是否会比一直默认恒等映射要好）。论文highway中提到了使用gate门

2.有效果的原因是用短接来引导整个模型(同时也避免梯度消失)

### 4. GPT

①标题+作者

②摘要

自然语言理解任务多样性强，比如有文本蕴涵、Q&A、语义相似性评估、文本分类等。对于这些任务，虽然有大量无标记文本，但是对于具体任务的有标记数据却很少。于其分门别类地去设计适用于每个具体任务的模型，我们提出了一个可以从大量无标记数据集上训练出可以只经过具任务数据的输入和模型的微调就能适配各个具体任务的模型。我们这一无视具体任务的模型在常识推理上提升了8.9%的精准度，在Q&A上获得了5.7%的提升，文本蕴含上则有1.5%的提升。

③结论



④导言

在使用无标号文本时遇到的困难：1.难以选定优化目标函数以获得更容易迁移的文本表示2.如何有效地将已经学到的文本表示迁移到具体任务上

本篇论文使用了半监督方法应用于语言理解任务，该方法包括了无监督的预训练和监督的微调。其目的是使模型学习到一个通用的可以简单调整久就能迁移到其他任务的表达。

在这一设定中，并不要求目标任务与无标记的语料库在属于同一领域。

整个训练过程分为两个阶段：第一步是使用无标记数据建模语言模型从而学习到神经网络模型的初始化参数。第二步，使用目标任务的有标记数据集对模型参数进行微调。

整个模型的架构使用的是Transformer，其在机器翻译、文本生成、句法分析等多个任务领域已展现出了非常好的性能。该模型提供了有利于处理文本中长周期依赖/关联的更有结构性的记忆架构。在迁移阶段，处理的结构化文本输入将被看作是一个连续的词元序列，该调整使得微调时只需要稍微修改预训练模型就可以取得很好的效果。

论文中的方法在四种语言理解的任务上进行了评测：1.自然语言推理 2.Q&A 3.语义相似性 4.文本分类

⑤相关工作

本篇论文工作从属于**半监督学习**(pretrain无监督+fine-tuning有监督)

GPT、Bert所使用的方法已经被称为**自监督学习**(self supervised learning)

此前研究人员已证明了，通过使用大量无标记数据集去训练word embeddings可以将其用来提高多种任务的效果。但是这一方法，只是传递了词级别的信息，本文意在获取更高层级的语义信息。

先前也有人探索了使用短语级或句子级的嵌入同样也是使用大规模无标签的数据，最后在那个把文本编码成对于多种任务合适的向量表示。

**无监督预训练**其目的是找到一个表达良好的初始化，在早期有应用于图像分类、回归问题中。后来的研究证明，预训练作用类似于正则化方法，使得整个模型泛化性更强。最近的工作中，该方法被用来帮助在图片分类、语音识别、实体歧义消除、机器翻译等领域训练深度神经网络。

与GPT最接近的工作是使用语言建模目标去预训练神经网络，然后在具体任务上进行有监督的微调。但由于其使用的是LSTM模型，从而使得性能受限。

GPT则使用了transformer神经网络，从而能捕捉到更长范围的语义结构。其高效性，在自然语言推断、释义检测、故事续写任务上得以验证。

还有一些方法在具体任务上进行监督学习时，使用了从预训练或者机器翻译模型中得到的隐藏表示作为辅助特征，该方法增加了大量参数。

添加辅助的无监督训练目标是半监督学习的另一种形式，辅助NLP任务的方法，有如词性标注、组块分析、命名实体识别和语言建模，以提高语义角色标注，GPT同样也使用了辅助对象

⑥模型

**无监督预训练**

对于所给的序列tokens，GPT使用标准的语言模型建模目标去最大化以下公式的可能性
$$
L_1(U) =
\sum_{i}
log P(u_i|u_{i−k}, . . . , u_{i−1}; Θ)
$$
k是上下文窗口的大小，概率P是使用有θ参数的神经网络建模出来的，θ所代表的参数，由SGD训练而来。

实验中使用了多层的Transformer解码器作为语言模型，该模型对经过向量编码前馈神经网络的输入tokens进行了多头的自注意操作，最终生成了对于目标tokens的分布。
$$
\begin{aligned}
&h_0 = UW_e +W_p \\
&h_l = transformer\_block(h_{l−1})\space ∀i ∈ [1, n]\\
&P(u) = softmax(h_nW^{T}_e)
\end{aligned}
$$
U是tokens的内容向量，n是层数，We是token嵌入矩阵，Wp是位置嵌入矩阵

**有监督微调**

在训练好好公式(6)中的模型目标后，GPT调整参数去适应监督学习的目标任务。对于带标记的数据集C，其中的每个实例都由一个输入序列$x^1,……，x^m$和标签$y$构成，该输入序列经过预训练模型最终由transformer块的$h^m_l$进行激活，然后经过线性输出层通过参数$W_y$去预测y：
$$
P(y|x^1, . . . , x^m) = softmax(h^m_l W_y).
$$
这就是的我们可以去最大化以下目标
$$
L_2(C)=\sum _{(x,y)}log P(y|x^1, . . . , x^m).
$$
作者发现：加入语言模型作为微调的辅助目标有利于提高监督模型的泛化性，并且能加速模型的收敛。具体来说，将以下作为目标：
$$
L_3(C) = L_2(C) + λ ∗ L_1(C)
$$
总之，在微调阶段所需的额外参数就是$W_y$，并且为分隔符进行嵌入。

对于某些任务，仍然还需要对其输入进行结构化比如Q&A任务、文本蕴含任务将句子对有序化或者构造文档、问题、答案的三元组。GPT使用遍历风格的方法，将结构化输入转变成一个有序的序列，这样预训练模型就可以处理它了。这样的输入转变，使得我们在处理不同任务时避免了模型架构上大量的改变

⑦实验

在无监督预训练阶段使用的数据集是BooksCorpus，其中不乏有长篇幅的连续文本内容，使得生成式模型可以学到长距离的信息。其替代品是ELMO使用的1B Word Benchmark，但它的缺点是将句子级的数据随机打乱了，从而不利于模型的训练

模型整体同transformer区别不大，训练了12层的decoder，使用了带掩码的多头自注意(768维的状态和12个头)。对于全连接层使用了3072维的内含状态，优化策略使用了Adam，最大学习率是2.5e-4，学习率在前2000次更新时从0开始线性增长并开始使用cos函数降低到0。使用了batch_size=64的随机选取的mini-batch进行了100个epochs的训练，其中每个mini-batch中有512个连续的tokens

在微调阶段，除特殊情况外，直接沿用了预训练时的超参数设置，并为分类器添加了参数为0.1的dropout。对于大多数任务，use a learning rate of 6.25e-5 and a batchsize of 32。

⑧评论

question:

1. 为什么在无监督预训练相关工作中说：预训练充当了一个正则化方案，使得模型有了更好的泛化性(不应该是无监督预训练为文本在多维空间中预选到了一块较为好的区域？如果以类似字型接近度同等地在计算机表达中也靠近，那样无标记的数据在训练完后将天然地在多维空间中靠近)
2. 为什么模型介绍3.2中加入语言模型作为微调阶段的辅助目标有助于a)提升泛化性b)加快了收敛
3. GPT会从多任务训练中获益？

### 5. GPT2

语言模型是多监督的多任务学习者

进行了多任务预训练使用超大数据集和深层模型，模型参数量达到了15亿个，并且在webtext的数据量下仍然欠拟合。

使用了zero-shot即在下游任务改造时不适用有标号的数据，使用该方法后在下游任务改造时不能出现模型从未见过的特殊符号，故需要新的方法对模型进行prompt提示

### 6. GPT3

对比zero-shot、one-shot、few-shot



<img src="论文精读.assets/1665931747476.png" alt="1665931747476" style="zoom:67%;" />



few-shot无法做到将上次的同类问题的提示记住，而是每次预测都要添加提示信息

局限性：1）长文本生成上比较弱 2）有结构和算法上的局限性 ，不能像bert一样能够往前看 3）每次预测下一个词时名不能分辨出已知词哪个更为重要 4）样本的有效性不够 5）多样本进行上下文学习时不确信其是否从头开始学，还是从之前所学样本中找出相关

## 对话生成

### 大规模中文短对话数据集LCCC

### (Large-scale cleaned Chinese conversation dataset)

①标题+作者

清华大学王义达  https://www.bilibili.com/read/cv8946802 、郑银河

②摘要

开放域短文本对话需要一个大规模高质量的数据集来训练，本文即提出了一个清洗过的中文开放域对话数据集LCCC，其含有一个base版本内有680万个对话，另外一个large版本，内有1200万个对话。数据清洗使用了严格的标准，该标准由一系列规则和一个在人工标注的11万个对话数据对上训练出的分类器组成。同时，作者也分别使用两个数据集在GPT2进行了预训练发布了两个预训练模型LCCC-base、LCCC-large。

③结论

④导言

BERT推动了自然语言理解类任务，GPT极大提升了自然语言生成类任务的精确度。

除了这些高效的基于Transformer的大模型，一个好的对话数据集同样很关键。尤其在开放域对话系统中，现存工作大都是从Twitter、Reddit、OpenSubtitles和其他公众资源爬取了英文语料库，并配合预训练模型，取得了很好的效果。

当前数据驱动的对话系统大都是基于公共平台资源或者众包数据集。公平平台资源规模大，但有大量噪声数据需要被清理；众包资源质量高但是数据量小。



LCCC中的数据由Weibo数据和其他中文语料库组成

但是中文预料库的缺少使得我们去预训练中文对话模型非常困难



本文所创建的LCCC可以作为开放领域中文对话生成的benchmark数据集

本文提供了两个预训练模型，其先在中文小说数据集上进行预训练，再使用LCCC进行后训练(post-trained)

⑤相关工作

前面说了公开的社交媒体资源来源，其来源广泛但所爬取的资源确实带有很多噪声的即脏数据。再就是基于众包的一些高质量数据集，这些数据集是为高级的对话任务而构建的，比如为基于知识的对话生成提供的数据集[wizard of wikipedia(WOW)](papers/wizard of wikipedia (WoW).pdf).、[document grounded conversations(DOG)]().为角色增强的对话生成构建的[PERSONA-CHAT](file:\\\). 以及为情感对话生成构建的[DailyDialog](file:\\\)

这些基于众包的数据集虽然质量高但是数量太少。

GPT出现后在很多文本生成任务上都达到了SOTA水平，预训练模型也就在对话生成领域火了起来。[DialoGPT](papers/(DialoGPT)Large-Scale Generative Pre-training for Conversational Response Generation.pdf) (其声称已经被[GODEL](papers/GODEL Large-Scale Pre-Training for Goal-Directed Dialog.pdf).超越）)即提供了一个在英文开放域对话的预训练模型，该预训练模型使用GPT-2在147millions个在Reddit上的对话进行了后训练

[Meena](论文精读.papers/Meena.pdf)使用有着26亿个参数的Evolved Transformer在大量英文社交媒体对话数据集上进行了预训练，该数据集中包含了400亿个单词。

Chinese GPT模型所使用的数据集是Chinese Wikipedia2 (17亿个 words)和Chinese News (92亿个 words)

本数据集使用的是从微博上爬取的7千9百万个对话，对些对话进行严格地清洗后得出了**LCCC-base**。 又在添加若干中文对话数据集后在相较base版较为宽松的清洗后得到了**LCCC-large**。整个清洗的过程包括基于规则和基于分类分类器的过滤

**数据收集**

LCCC-base收集数据分两阶段，首先是选取种子用户，其中有一些是手工选取自有关注专业的大众新闻媒体的账号。之后我们认为在这些新闻下发表评论的用户都是高质量用户，因为机器账号通常对这些日常新闻并不感兴趣

第二阶段，收集这些被选中用户的对话。这些用户所发的微博包括其下面的评论都被收集起来构造成树结构。其中任意一个根到叶子的路径都是一个对话，对其使用深度优先搜索进行重构后就得到了7千9百万个对话原始文本。

LCCC-large 先是从Chinese Chatterbot Corpus、PTT Gossing Corpus、Subtitle Corpus、Xiaohuangji Corpus、Qingyun Corpus and Tieba Corpus收集了单轮对话数据。收集了 Douban Conversation Corpus , E-commerical Conversation Corpus and a Chinese chat corpus 作为多轮对话数据，在混合上先前79million个对话后使用相较LCCC-base较为宽松的清洗规则后得到LCCC-large

**清洗过程**

基于规则的：

1）删除一些特殊符号 2）将30轮以上的对话分割为多个少于30轮的对话3）删除回复太长或太短的对话4）对于重复6遍以上短语或词只维持一个副本5）删除识别为广告的对话6）删除带有万能回复的对话

同时也将一些噪声添加进了黑名单1) dirty words,sensitive words, and dialect; (2) special topics words such as levofloxacin; (3) name, appellation and unknown abbreviation; (4) special symbols and emoji; (5) platform signs such as ads, pictures, and videos related words

基于过滤器的：

很多语义语法类层面的和一些依赖于上下文的对话很难被基于规则的方法过滤掉。因此本文使用了两个BERT分类器进行了进一步过滤，综合评价精准度、召回率和F分数等可信度高的分数选择了一个最可信的过滤阈值

第一个BERT分类器在手工标记的10万个对话上进行训练。对话被标注为有噪声的规则：(1) The response is not fluent or there are serious typos in the sentence, (2) The information of the response is incomplete; (3) The topic of dialogue is time-sensitive, (4) Festivals, places, gender and time which are not mentioned in the post appear in the response (5) The post and the response are irrelevant.下图为例子

<img src="论文精读.assets/1667455225016.png" alt="1667455225016" style="zoom:67%;" />

该分类器的精度在测试数据集上达到了**73.76%**

社交媒体中的一些对话往往涉及超出文本外的语境，这些看似无主题的对话也是应该被过滤掉的，第二个BERT分类器即做这个工作，其分类精度在测试数据集上达到了**77.60%**。

**统计数据和结果**

Avg.words代表每句话里单词的平均数量，文本的分词工作是使用Jieba进行的。本文用自己的黑名单评估了STC(single-turn short text conversation)数据集的噪声级别，结果显示其中60%的对话有dirty words, sensitive words, special symbols, etc. 使用STC训练出的模型相较于在LCCC上训练的生成的黑名单词汇有五倍多。以下为LCCC数据集相关统计信息

![1667457638779](论文精读.assets/1667457638779.png)

LCCC数据集与现存其他数据集对比图：

![1667457672465](论文精读.assets/1667457672465.png)

⑥模型

**模型架构**

![1667467662720](论文精读.assets/1667467662720.png)	

>[GPT2](papers/GPT2.pdf)对于[GPT](papers/GPT.pdf)的修改
>
>Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/
>√ N where N is the number of
>residual layers. The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used.

模型部分使用的是基于transformer的GPT，decoder使用的是带掩码的多头自注意力块

对于给定的回复和历史对话信息，y = (y1, ..., yL)和U = {u0, ..., un}对于生成的U($u^1_{n+1}, ..., u^{j−1}_{n+1}$) 通过利用最大近似估计maximum likelihood estimation $∏^L_{j=1} P(y_j|y_1, ..., y_{j−1}, U)$去生成$u^j_{n+1}$，直到遇到结束符截止

同[TransferTransfo](papers/TransferTransfo A Transfer Learning Approach for Neural Network Based Conversational Agents.pdf)一样将当前所说的语句与历史对话信息一同concate到了一个长文本序列。

>a sequence of input tokens for the model is constructed for each utterance by concatenating all the persona sentences of the current speaker (usually 4 to 6 sentences in the PERSONA-CHAT dataset) with a history of the dialog’s previous utterances (typically 3 to 5 previous utterances).

模型输入是word embedding, speaker embedding, 和position embedding的总和，word embedding、position embedding是在预训练阶段所学到的，speaker embedding则是在后训练或者微调时学到，speaker embedding用来表示不同的讲话人。同Bert一样将[CLS]作为序列的开始标记，[SEP]作为序列的终止标记

![1667465797551](论文精读.assets/1667465797551.png)

>(i) a personality sentence, (ii) an utterance from PERSON1 or (iii) an utterance from PERSON2. These additional embeddings are learned on the PERSONA-CHAT dataset during the fine-tuning phase.

同[DialoGPT](papers/(DialoGPT)Large-Scale Generative Pre-training for Conversational Response Generation.pdf)一样，整个模型是在一个中文预训练模型GPT_novel上使用我们的数据集进行的post-trained

在处理多轮对话时同DialoGPT一样 我们将对话中，从第二句到最后一句每句话作为历史句的回应(?不太懂)

具体模型以及训练次数![1666516405298](论文精读.assets/1666516405298.png)

**post-train的参数设置**

对于所有模型都选用AdamW(Adam对学习率不敏感的非常平滑的SGD，因此不必太多地去调参，效果与SGD+momentum差不多)为优化器，Noam作为学习率衰减方法，层数都为12层，注意力头数12，word embedding维度为768，position embedding 维度 513，最大学习率6.25e-5，batch_size=8，梯度累加=64

⑦实验

**微调设置和具体实现细节**

为了评价这些模型，将他们在STC数据集(contains 4.4M conversation pairs)上进行微调,将其随机划分为了训练、验证、测试集，测试验证集各有不交叉的2000个对话

>STC
>
> 这是一个基于Sina微博的数据集，是从一些中国搞NLP的高级知识分子的微博posts中爬下来的（posts的质量较高），但是comments（replies）是所有人都可以发的。 

本文使用的baseline：[GPT2chitchat](https://github.com/yangjianxin1/GPT2-chitchat)(本文写作时唯一一个中文对话预训练模型2019.12.9,基于GPT2与训练了50w个中文对话)、Vanilla transformer、Attn-Seq2Seq

GPT_novel微调了30个epoch，其他模型(CDialGPTLCCC−base、CDialGPT2LCCC−base、CDialGPTLCCC−large)微调了10个epoch。微调时使用相同的batch_size和相同的梯度累加数，其他的超参数也更后训练阶段设置相同。

transformer and Attn-Seq2Seq不经过预训练在STC数据集上训练直到其收敛(consists of 6 layers of GRU with Bahdanau attention mechanism the dimension of hidden states is set to 768,The layers of the transformer are also set to 6，以使其与本文的预训练模型具有差不多的参数量)

Bahdanau attention 架构如下：

![1666880206930](论文精读.assets/1666880206930.png)

对于所有模型学习率都使用AdamW优化器从6.25e-5 线性衰减到0

CDialGPTLCCC−large在STC上微调后的生成样例：

![1667484083999](论文精读.assets/1667484083999.png)

人机交互对话样例和自交互对话样例：

![1667484217031](论文精读.assets/1667484217031.png)

 所有的回复均使用 [Nucleus Sampling](papers/Nucleus Sampling.pdf) 的方法采样得到 (p=0.9, temperature=0.7)。 

**模型评估**

自动评测：使用BLEU和distinct n-grams，

由于BLEU不能很好地反映生成结果的质量，本文又采用了Greedy Matching来评估帖子和生成的回复之间在单词级别的相关性，以及使用Embedding Average来评估其在句子级别的相关性。除了词汇量与其他模型不同的GPT2-chichat外本文还给出了模型的困惑度，评估数据如下：

![1667491085727](论文精读.assets/1667491085727.png)

人工评测：

对于三个评分员每个模型提供200个生成样例，从流畅性、相关性和信息量三个角度来打出2/1/0分，打分结果如下：

![1667491263272](论文精读.assets/1667491263272.png)

本文还使用了Fleiss kappa来评估评分者间的意见一致性，打分结果为0.39-0.44之间表明了较好的一致性。

⑧评论

___

### GODEL : Lare-Scale Pre-Training for Goal-Directed Dialog

①标题+作者

- 微软雷德蒙德研究院
-  哥伦比亚大学 

，一作[pengbaolin](https://www.microsoft.com/en-us/research/people/bapeng/)

②摘要

GODEL使用了新的基于知识的预训练方法来使模型更好地支持那些，需要对当前对话引入外部知识的下游任务来使其产生更好的回复。实验对比了一系列基准模型，包含任务导向的对话、问答对话、需要基础知识的开放域对话，结果表明GODEL在人工评测、自动化评测上都达到了当前预训练对话模型(微调阶段使用few-shot)最好的水平(SOTA). 在本文评估方法中，新增加了一个特征，即**utility**有用性这个概念(即评估回复的有用性[extrinsic evaluation]，而不是只关注于其交流过程中相关的特征[intinsic evaluation]，即流畅性)。引入有用性特征后，更有助于提高评分者间意见的一致性以及自动化评分与人工评分的相关性。

③结论

④导言

GODEL是为general-domain一般域对话设计的完全开源的预训练模型。其有两大亮点：1.将预训练分为了三个阶段，

 1) Linguistic pre-training on public web documents to gain the capability of **text generation**. 

2) Dialog pre-training on public dialog data to learn to **chat like a human**.

 3) Grounded dialog(有基础的对话) pre-training to enable a dialog model to generate responses grounding on specific goals. i.从web文本中连续折叠数据ii.使用如Reddit之类的公开对话数据iii.一系列现存的支持基础对话任务的语料库

其中带有外部知识的对话语料库包括MS MARCO、DSTC7，其能使得微调时有效帮助到那些需要产生有关外部知识的回复。

2.GODEL的验证是有用性驱动的，使用的是一套为**开放领域任务导向的通用对话模型**(open-ended goal-directed general-domain dialog models)的少样本微调设计的基于有用性的基准。使用了该有效性检验方法后，GODEL对于任务导向对话的微调效果比其他大型的预训练语言模型要好。

没有一个鲁棒性的自动评估标准一直是多用途开放对话模型的一大问题，最近的一些SOTA预训练模型，往往因为缺少一个认同度高的评价标准而没有进行有意义的比较。

而要制定一好的标准，我们则要回归初衷，不应是只关注对话回复的流畅度和其社交能力这一从交流来看的内在维度，而应该关注其有用性这一外在维度，即对话系统所生成的回复应对用户有用才行。本文认为该功能有用性的外在维度是**general-domain**模型更适合的自动化评价方法

本文用GODEL在四个任务上进行fine-tuning，并探索有用性这一概念，这些任务有：任务导向的MultiWOZ、开放域目标导向的任务比如CoQA、Wizard of Wikipedia和Wizard of the Internet

GODEL相比于baseline DialoGPT在目标导向的任务上做的更好，对于在不同任务上的结果证实了本文方法的有效性。并且当聚焦于目标导向和外在评价时，评分者间意见更具有一致性且评估指标更具有相关性。

本文提供了多个版本作为未来工作的baseline，$GODEL_B$、$GODEL_L$、$GODEL^{GPT-J}_{XL}$

⑤相关工作

大规模的预训练模型对对话领域已经产生了巨大影响，比如DSTC、ConvAI就已经在竞赛中取得了好成绩，并且在任务导向、闲聊方向都得到了广泛应用。然而对于这些预训练模型的实验评估仍局限在内在的如Relevance、Informativeness、Humanness(human-like)还有各种基于字符重叠的自动化评估标准，DialoGPT提供的基于Reddit的预训练模型也即如此评估的。[Meena](papers/Meena.pdf)的评估着眼于Sensibleness合理性和Specificity明确性，同时也提供了一个新的自动评估方法SSA，从这两个维度去进行评估。[BlenderBot](papers/BlenderBot 3.pdf)的模型利用了各种技术包括人性化(human-like)、共情化、知识化，但其评估核心却是人性化(human-like)和新颖度。Plato-XL使用内在标准：Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness。而Plato-XP在评估DSTC9-Track1、MultiWOZ 2.2、DuConv时使用了一些外在标准，但是都是使用的基于具体任务的自动评估(**ROUGE-L** for DSTC9, **Goal-Accuracy** for MultiWOZ, and KnowledgeF1 for DuConv)。

本文则是提供了更为统一化的对有用性的评测标准，即用统一的方式比如相同的注释结构

LaMDA在进行初步评估时使用了内在特征，但在人工评测阶段考虑了外在特征即有用性，这是同本文工作最接近的一个。

本文则是想通过对比外在内在特征，分析他们与具体任务和数据集的自动化评分的相关度。

作者又具体解释了

**Open-Domain Goal-Directed Dialog**

本文在目标导向这一基础上，想去生成更为自然且更具有用性的回复。

先前的一些任务导向的对话中已经用了大量的有用性的评测标准比如**Inform-rate** and **Successrate** for MultiWOZ，**Knowledge-F1** for Wizard of Wikipedia，这些**定制化**的评价标准往往具有异质性，并且只能应用于有限的子任务，这使得分析跨对话任务和数据集的结果较为困难. 本文解决这一局限性的方法为，提出了一个人工评测的统一标准，比如当一个评分员被要求为一个餐馆领域的对话系统进行评分时，他对于能给出推荐或者给出一些餐馆信息的系统所给的分数就应该比一个只会闲聊的系统要有更高的有用性分数。

有用性在任务导向对话中的普适程度就相当于在闲聊对话中的人性化(human-like)和有趣性，但其统一化的评分标准却很少被用在目标导向的对话系统中。

作者进一步对比了人工评分下的外在、内在评价标准，并且将其与自动化评分标准联系起来。

首先展示了使用Krippendorff’s alpha方法计算的评分者间意见一致性，该方法适用于Likert量表的众包评价，其特点为

(1)评价者的个数任意

(2)不同评价结果的数量任意

(3)可以为不同种类的评价结果设定不同的差异衡量标准

(4)允许数据不完整

(5)对于大数量数据和小数量数据的处理方法相同

![1668244462177](论文精读.assets/1668244462177.png)

如上说明有用性这一外部评测标准更具评价者间意见一致性。其中WoW数据集的内在特征评分者间意见一致性更高，其原因是该数据集中的对话更多地依靠了闲聊，并且在改定的语境中并不总是具有一个有用的概念占主导。CoQA数据集有用性的意见一致性低是由于其给出的总是短的基于事实的回复，这使得评分者不易去区分系统的回复。

之后作者又使用**斯皮尔曼等级相关系数**(关注两组数据的单调性，换句话说是两组数据的趋势 )分析了以上人工评价和一些自动化评价方法标准的相关程度。自动化评价方法使用了BLEU、BLEURT、BERTScore和chrF，

>The chrF metric is a lexical-match metric similar to BLEU, but is character-based rather than word-based and but has been found to be more robust than other surface-level metrics 

结果如下

![1668248085286](论文精读.assets/1668248085286.png)

由于区分开放式回复生成的内在和外在评分的质量这一做法还是比较新的，所以本文试图找到一些更适于评价这些不同特征的评价方法。

从上表可以观察到，这些评价标准往往与外在特征的评价更加相关。这一结果也是由于外在特征具有更好的评分者间意见一致性导致的。

并且可以发现基于语言模型的评价标准(BERTScore and BLEURT)在相关性上不如词汇匹配的评价标准，这是由于其关注于语言建模于是自然地将焦点放在了评测人性化(即更像人，e.g., fluency and well-formedness)，这就使得其对于有用性的辨识能力下降。作者对于chrF对于外在特征更有相关性的原因上并不是很清楚，但其推测在比如QA和基于知识的需要精准地获取事实信息比如年份或具体人名而不需要对其加以阐述。

作者有趣地发现安全性与自动化评分标准有着不错的相关性。任务导向对话中(MultiWOZ)，安全性与chrF有最好的相关性，在偏闲聊的对话中(WoW)则BERTScore与人工评测相关性更好。作者认为这一不错的相关性是由于对话中所引用的内容通常很安全。

这些评论者间意见性和相关性结果表明对于**开放式对话系统**采取外部评测更好。不论对话系统是否与具体的任务联系在一起，对话总是有一个或多个目标的，评判这些逐步实现目标的回复的有用性是自然且可取的。这些结果也同样说明了向外在特征转变可以使得对话自动评估不那么具有挑战性(即拉近了自动化与人工之间的距离)。对于Humanness(human-like)这一内在标准的评估主要还是保证对话系统不牺牲人性化(human-like)和安全性从而变得更为有用。

考虑生成一个具有高有用性的回复，作者将open-domain goal-directed 对话生成任务定义为：对于给予的源对话上下文$S=（s_1,……,s_N)$,以及环境$E$，目标是生成目标句子$T=(t_1,……，t_N)$.$P(T|S,E)$的条件概率可以被表述为一系列条件概率的产物：<span name='godel-eq1'>eq1</span>
$$
P(T|S,E)=∏_{n=1}^Np(t_n|t_1,……,t_{n-1},S,E)
\tag{1}
$$
$E$代表world的状态和外部知识(e.g., a database or results of a search engine)也即超越闲聊具有有用性所需的。在预训练阶段，$E$通常是不存在的，因为E往往是对于特定任务来说的，但是作者发现在预训练的第三阶段使用基础知识文本替代E有很好的效果，i.e., grounded dialog pre-training.

⑥模型

![1666616731830](论文精读.assets/1666616731830.png)

GODEL是在web文本上训练的标准的预训练语言模型。其使用了seq2seq的Transformer模型，如上图，根据给定的历史对话和环境去给出回复。下图是一个训练样本例子：

![1668080782761](论文精读.assets/1668080782761.png)

在预训练的第二个阶段即广域对话预训练，使用的数据集是从DialoGPT的Reddit评论链中摘取的。(which consists of 147M dialog sessions for a total of 6B tokens.)

预训练最后的有知识基础的对话，使用的是现有的支持基于知识生成回复的、传统的对话Q&A的和任务导向的对话数据集。

DSTC7 Task 2、MS MARCO、UnifiedQA、The Schema-Guided Dialog

本文构建了三个不同大小的模型1）220M个参数的基础版本$GODEL_B$，2）770M参数的版本$GODEL_L$，有175B个参数的$GODEL_{XL}$

基础版$GODEL_B$有12层encoder和12层decoder，其词嵌入的维度有768维。$GODEL_L$的encoder、decoder的层数翻倍，并且词嵌入有1024个维度。这两个模型是分别从T5和T5-Large初始化来的，并且基于HuggingFace仓库的版本。本文使用了HuggingFace中实现的BPE字节级别分词。

$GODEL_{XL}$是从GPT-3初始化来的，尽管由于版权原因无法发布，作者还是想在最好的模型上测试自己基于知识的微调方案。为了代表$GODEL_{XL}$，作者发布了基于GPT-J的一个预训练版本。

![1668087889478](论文精读.assets/1668087889478.png)

如上图，该替代版本的功能与GPT-3版本是相当的。

⑦实验

本文评估了两种配置的微调模型，few-shot和full。关注焦点在few-shot上，这是由于任务导向的对话数据集(e.g., MultiWOZ)构建的成本很高，并且比闲聊数据集更小，因此本文想评估微调样例相对少的情况下模型是否仍能表现地很好。并且这篇工作本身就是专注于对话领域的预训练，few-shot少样本学习能更好评估该预训练模型的效能。再者，少样本微调是现实应用场景中更为实际的方法

GODEL对于开放域目标导向对话任务只需要很少的带标签的数据样例去微调。本文通过在三种目标导向的对话(i.e, 知识基础的回复生成、任务导向对话、对话问答)数据集上进行微调来评测GODEL。

具体的数据集有Wizard of Wikipedia、Wizard of Internet、MultiWOZ、CoQA

对于few-shot微调，将从这些数据集中为每个任务的微调随机采样50个对话，并且使用它们原本的测试集用来评估。GODEL在每个任务上的微调使用的是相同的预训练配置，使用[公式1](#godel-eq1)

作为训练目标。最后基于在验证集上的困惑度分数选择最佳模型。

**Baseline pretrained models**

本文与T5、BART、DialoGPT和BlenderBot这几个预训练模型进行了比较。具体微调使用的是For T5, we fine-tune from both T5-base (T5B) and T5-large (T5L). For BART, we fine-tune from both BART (BARTB) and BART-large (BARTL). For BlenderBot, we fine-tune from BLENDERBOT400M, which is distilled from a 3B model.

**Automatic evaluation metrics**

基于评测有用性的角度，使用了一下评测函数

$F_1^R$评测预测与基于知识回复间的平均重叠

$F_1^K$评测模型回复与数据集中知识的重叠

$Inform$评测模型是否提供了能够满足用户的充足的信息

$Success$评测模型提供的信息是否包括所需的所有特征。

$The \space Combined \space score=（Inform+Success）*0.5+BLEU$

用来评价总体质量

除了以上有用性函数之外，本文还使用了BLEU、BLEURT、BERTScore和chrF

BLEU的分数基于corpus-level BLEU-4

 在表1至表5中，使用对最佳竞争对手的配对双侧t检验计算显著性。 

**Human evaluation setup**

使用轮次级别的人工评测去检验GODEL生成的回复是否1) useful, 2) human-like and 3) safe

使用Amazon Mechanical Turk众包平台雇佣了高素质评测员。首先向评测员展示对话历史、相关知识和由不同的系统(随机选择)生成的两个回复。之后让评测员考虑以下三个问题：

- 哪个回复更有用Extrinsic(i.e., contributes to making the conversation productive, especially towards achieving any stated goals)
- 哪个回复更像人说的Intrinsic(e.g., coherent, fluent, and natural)
- 哪个回复更具安全性Safety(e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)

以上在一个满分为5分的Likert scale上进行评分。

**Automatic Evaluation Results**

所有任务在少样本微调下的结果：

![1668344896062](论文精读.assets/1668344896062.png)

全部样本参与微调：

![1668344935066](论文精读.assets/1668344935066.png)

需要注意到少样本微调下GODEL比T5好很多，但是全样本下则相差不多，这是由于GODEL使用与T5一样的模型进行训练，若两个模型都微调至收敛则会相差不大。DialoGPT、Blenderbot这些在闲聊语料库上训练出的模型表现得差是在意料之中的。

GODEL在基于知识生成的任务上(WoW、WoI)的**内在评估**分数有所提升(结果整合到了一个表，如下：)

![1668348177313](论文精读.assets/1668348177313.png)

MultiWOZ and CoQA在外在评测上有所提升

![1668348165986](论文精读.assets/1668348165986.png)

![1668348195646](论文精读.assets/1668348195646.png)

本文结果显示在少样本微调情况下，GODEL的BLEU分更高并且在$F_1^k$分数上与baseline模型保持了相同水平。

表4中GODEL的**有用性评分(Success)**比T5高了23.6，并且BLEU高了8.5. 表5中作为有用性评分函数的$F_1^R$关注回复的正确性。总之结果表示GODEL在少样本学习的微调设置下的有用性分数比baseline要好。但是BLEU分数并没有得到提升，这是由于CoQA的回复通常很短

 本文构建目标导向对话模型的方法适用于不同的PLM，例如T5和GPT-3 

![1668349743290](论文精读.assets/1668349743290.png)

上表中可以观察到$GODEL_{XL}$比$GPT-3$性能好了很多。这说明带有基础知识的预训练对于GPT-3在任务导向的对话上的应用是非常有用的。

GODEL预训练阶段的消融实验：

![1668354158334](论文精读.assets/1668354158334.png)

表中数据说明，只去进行对话预训练会使得模型变差。这可能是因为模型仅仅是学习了进行对话，而下游任务都需要基于知识去生成的能力。

**Human Evaluation Results**

由于GODEL和T5在自动评分时表现最好，因此本文拿它们两个进行比较。并且GODEL是从T5初始化来的，比较两者能直接评测基于知识预训练方法所带来的影响。本文的人工评分是从所有任务中随机选取共4137个样例来进行评估的

![1668354562820](论文精读.assets/1668354562820.png)

上表中展示的是总的胜率，GODEL在三种指标上都胜过了T5

 值得注意的是，GODEL在CoQA上的有用性得分与T5相似，但在对这项任务的人工评估中具外在和内在特征得分却更高，这可能是因为CoQA的目标比其他任务更明确 

⑧评论

thinking: 各领域现在的评价标准一定是最合适的吗，有哪些领域的亟待修改？修改评价标准后对领域内模型的发展趋势，以及模型进步的速度是否有影响？评测标准注重于有用性就是好的吗？

本文没给出第一阶段预训练的数据量？

分阶段递进预训练是必要的？

A survey：different works' evaluation methods and their correlation with attributes?

question：Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.？？

**文末附录中有人工评测具体表**





---

### Open-Prompt

①标题+作者

一作丁宁清华大学计算机科学与技术系博士生。师从郑海涛教授，也得到了刘知远教授的共同指导。主要研究自然语言处理和机器学习。尤其是知识表示、提取和应用。

②摘要

提示学习已经称为NLP新范式，即直接调整预训练模型来完成完形填空风格的预测、自回归建模或者序列到序列的生成。然而对于提示学习的应用仍没有一个具体的框架。现存的提示学习代码库，通常是无规范的，只能局限地应用于某些具体的应用场景。导致该情况的是由于一些比如：无法统一的模板化策略、初始化策略和verbalizing strategy等, Open-prompt即提供了统一的方法，**其组合能力允许将不同的预训练模型自由组合**，具有很强的可扩展性 。

③结论

④导言

预训练语言模型引领了NLP的新时代，早期，调整预训练模型使其能完成各种具体的NLP任务时使用预训练+微调的范式，其需要额外的参数和具体任务的训练目标(损失函数)。但从T5和GPT-3开始，研究者发现文字提示或说明对于PLM有很好的引导作用。

以情感分类为例，需要设计一个模板和标签-单词映射器verbalizer，模板即文本+标识符号。假设template是"<text> it is <mask>"其中<text>代表原始的文本，此处映射器verbalizer是{“positive”:“great”, “negative”:“terrible”}

句子“Albert Einstein was one of the greatest intellects of his time.” 将会被预定义的模板包装为“Albert Einstein was one of the greatest intellects of his time. It is <mask>”，包装后的句子将经过分词后输入PLM去预测<mask>上可能出现单词的分布，从原始文本中可直，此处我们希望得到‘’great‘’的概率比“terrible”的概率大一些。

之后围绕着如何构建模板、映射器、优化和应用来对提示学习这一新范式展开了研究。

提示学习可以被认为是与训练语言模型、人类先验知识和具体NLP任务的综合。现有的深度学习或自然语言处理库并不能很好的支持提示学习的应用，并且也没有一个基础的规范。先前工作都是在最有效的传统微调的方法框架上做最小的修改，导致了很差的可读性甚至难以复现。尤其是提示学习的表现受模板和映射器选择的影响极大。目前也没有一个为提示学习设计的开源框架，使得新旧工作进行严格对比非常困难。

本文一大卖点就是设计的提示学习框架中支持多种任务格式(分类任务和生成任务)、PLM(MLM、LM和Seq2Seq)、提示模块(不同的模板和映射器)的灵活组合。比如我们可以非常便易地将前缀微调与文本分类任务相适配。OpenPrompt这一特性可以使得用户在不同任务上评测自己提示学习模型的泛化性，而不局限于具体某个任务。

OpenPrompt具体设计有：Template类、Verbalizer类、PromptModel类

Template中为实现多种模板的统一范式，设计了新的模板语言，该语言可以实现为对应特征进行词元token级别的定制化，比如用户可以定义哪些词元是共享词嵌入的、可训练的或这些词元应进行怎样的后期加工处理

⑤相关工作

传统的微调面临着两个问题：1）该方法会在预训练与微调之间产生一个鸿沟2）随着模型参数量的增加，微调的可操作性变得越来越小

⑥模型

**Combinability**

在Open-prompt的框架下，从模型观点来看，T5将不再仅用于预测寿命，GPT将不再只用于生成任务

从提示学习的角度来看，前缀调优将可以用来进行分类任务，软提示也将能用来完成生成任务。

**Pre-trained Language Models**

由于提示学习意在通过减少预训练与下游任务应用之间的区别去提高精度，因此介绍了不同的PLM

预训练语言模型分为三大类：MLM、自回归风格的LM、seq2seq模型(常见的有T5、MASS、BART)

OpenPrompt支持直接从huggingface transformers直接加载预训练模型

**Tokenization**

在设计好模板后，原本具体应用于具体的输入以及模型的分词方法，将会变得耗时且有错误率

其封装好的数据处理API可以同时处理输入和模板，组件对从输入和模板融合的复杂信息进行分词，根据预训练语言模型选择的不同，OpenPrompt自动选择合适的分词器

**Template**

模板模块将原始文本与文字的或是软编码(可训练的)模板打包在一起，OpenPrompt中所有模板都继承自一个基类其中含有共通的特性和抽象方法

先前对于模板的设计，有人工编写的和纯软(可训练的)的模板。后出现的二者的混合版本效果有时会更为好一点，最近的一些工作中的提升是通过修复大量的手工词元，同时调优一少部分其他词元来实现。

模板语言借鉴了python中dict字典的语法特点，这样设计可以同时保证灵活性和简洁性

具体设计为一个模板节点由一个text和一个特征描述组成。在模板语言中，每个词元的特征是可编辑的，如设置一些特征共享embedding，设计特征如何进行后处理。

![1666957558977](论文精读.assets/1666957558977.png)

**Verbalizer**

标签单词映射器在分类任务中是必不可少的

**PromptModel**

该类实例化的对象负责训练和推理，它包含PLM、模板对象，和一个标签单词映射器(可选)

基类中使用了一种模型无关的前向传播方法去预测应该加掩码的单词位置，从而可以无视预训练中的目标函数，只调用API就能”预测需要预测的位置的单词(即找到合适的加mask的位置)“

**Training**

从参数可训练角度出发，提示学习的训练可以分为两种策略：1）同时调整提示和预训练模型 2）只调整提示的参数，而预训练模型保持不变

其中第2个方法被认为更有前景

⑦实验

⑧评论

---



### Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey

①标题+作者

南洋理工大学

②摘要

现今对话系统都SOTA都是基于深度学习方法的，本文从两个角度出发：1）模型类型2）系统类型 来探讨目前SOTA水平的对话系统。从模型类型角度，探讨了principles, characteristics和广泛应用的具体模型。

从系统角度出发，探讨了1)任务导向型 2)开放域 对话系统也即两个主流的研究方向。

本文也回顾了对话系统的评估方法和数据集

同时也指出了可能的发展方向

③结论

④导言

如今基于深度学习聊天机器人已经不再死板化。其市场规模已经从2021年的26亿，增长到了94亿，且有80%的公司希望在2021年年底之前配置上自动聊天机器人。

根据对对话系统的不同应用将其分为了两大类：1）任务导向的对话系统(task-oriented dialogue,**TOD**) 2）开放域对话系统(open-domain dialogue,**OOD**)。

TOD解决了一些具体领域的具体问题，如预定电影票，预定餐馆等。

ODD则注重与用户不限于具体领域的，不以任务为导向的聊天，通常是以数据为驱动的。

TOD、ODD都可以被看作是一个从用户发送的信息$U = {u^{(1)}, u^{(2)}, ..., u^{(i)}}$到智能体回复$R = {r^{(1)}, r^{(2)}, ..., r^{(j)}}$的一种映射:$R =ϕ(U)$

在一些TOD、ODD中该映射又添加了外部知识/数据而变为$R =ϕ(U，K)$,下图即为例子

![1667292677237](论文精读.assets/1667292677237.png)

有些数据集为每一个对话对提供外部知识标注，比如在TOD中的外部知识即可以从数据库中获取；ODD中的外部知识则可以从常识知识图中检索到

传统的TOD由分为四个功能模块的流水线结构构成：1）自然语言理解(NLU)2）对话状态追踪3）规则学习4）自然语言生成(NLG)

有一些SOTA任务导向的对话系统使用的是端到端的设计达到了比流水线设计更好的效果。

ODD则通常被分为了三种：1）生成系统2）基于检索的系统3）集成系统

1）使用的是seq2seq模型将用户的消息和对话历史匹配一个可能之前没有出现在对话语料库中的response序列

2）则是从之前已经存在的回复集合中选择一个进行回复

3）用两种方法结合了生成方法和基于检索方法：i.检索到的回复与生成的回复进行比较，选其中最好的 ii.生成式的模型用来优化检索到的回复

1）2）优缺点：

生成式的系统可以产出灵活的且上下文相关的回复，然而有时却缺少相关性并且常常产出一些傻傻的回复

基于检索的系统可以从人类回复集合中进行选择，因此在语言表面级别上更有相关性。然而检索系统受限于整个回复数据集合的有限性，并且有时检索回复的内容显示出了很弱的上下文关联能力。

先前的对话系统综述除失去时效性外，其中大多数都没进行多角度的分析。

一些过时的说法如chi-chat dialogue systems已经被替换为了open-domain dialogue systems.

传统的基于规则的对话系统由于其易于实施，并且可以自然回复，使其流行于先前的产品中。然而该对话系统应用领域是局限的。

基于非神经网络的机器学习对话系统通常通过模板填充

> Template filling is an efficient approach to extract and structure complex information from text to fill in a pre-defined template. They are mostly used in task-oriented dialogue systems.

来管理具体任务。相较于基于规则的，由于对话流没有被预先决定，而具有更好的灵活性。但是，其在F1模板填充分数上分数并不高，并且由于模板的固定受限于应用场景和回复的多样性。

Most if not all的SOTA模型都是基于深度学习的。

除对话系统之外，NLP领域与对话相关的任务包括但不仅限于1.问题回答2.阅读理解3.	 Dialogue Disentanglement (对话解纠缠： 其目的是能够在多人对话场景下，将一个完整且复杂的对话从数据流中分离为多条基于相似主题的线程，以便每条单独的线程都与特定主题有关。 ) 4.可视化对话5.可视化Q&A

6.对话推理7.对话语义解析8.对话关系提取9.对话情感分析10.仇恨言论检测11. MISC检测, etc.

整篇survey的篇章结构如下

![1667301878678](论文精读.assets/1667301878678.png)

**对话系统中的神经网络模型**

介绍的模型有：1）CNN 2）RNN 3）Vanilla Sequence-to-sequence模型 4）Hierarchical Recurrent Encoder-Decoder(HRED) 5）记忆神经网络 6）注意力神经网 7）Transformer

8）Pointer Net and CopyNet 9）深度强化学习模型 10）Generative Adversarial Networks (GANs) 11）知识图谱增强的神经网络

**CNN:**

现今的前馈层仍有一些问题：i.前馈层或是多层神经网络的操作仅仅是模板匹配，无法考虑具体的数据结构 ii.多层神经网络的全连接机制导致了参数的爆炸，从而导致了泛化性方面的问题。早期版的CNN，LeNet即缓和了以上问题。CNN一般由：卷积层、池化(聚集)层、前馈层.

近年NLP领域对CNN的使用急剧增多，许多任务**将词作为基本单元**，短语、句子甚至是段落对于语义表示同样有用，于是CNN就成了理想的语言分级模型化的工具

CNN是很好的文本特征提取器，但不是理想的序列编码器。

[Conversational Word Embedding for Retrieval-Based Dialog System](papers\Conversational Word Embedding for Retrieval-Based Dialog System.pdf)直接使用CNNs作为语句或者知识的编码器，但是大多数SOTA对话系统比如[Span-ConveRT Few-shot Span Extraction for Dialog with Pretrained Conversational Representations](papers\Span-ConveRT Few-shot Span Extraction for Dialog with Pretrained Conversational Representations.pdf).选择使用CNNs来作为文本信息编码后的**分级特征提取器**，提取时有直接基于encoder输出的特征向量的，也有从字节级别的embedding提取的，比如[ Multi-Hop Paragraph Retrieval for Open-Domain Question Answering ](papers\Multi-Hop Paragraph Retrieval for Open-Domain Question Answering.pdf)

有一些基于检索的对话系统，使用不同的encoder去编码对话上下文和候选回复，之后用CNN作为从编码后的对话上下文和候选回复计算出的相似矩阵的提取器

现今工作不常用CNN作为对话的编码器是由于其不能连续灵活地提取跨越时序的时间步序列(即缺少跨度较大的相关信息)

CNN在处理序列时有两个局限性：1）其假设每个数据点都是相互独立的 2）其输入通常是定长的

HMMs隐马尔可夫模型是传统的序列模型，但由于其推理算法的时间复杂度以及状态转移矩阵随着各个状态空间的增加变得过于大，并不能使其具体应用于有着大规模可能存在的隐状态的问题。并且其隐状态仅受临近的隐状态所影响同样也限制了模型的能力。

RNN则很好地解决了这一问题

The inductive bias of recurrent models is non-replaceable in many scenarios

现今的RNN模型可被归类为两大类：
1）Jordan类型的2）Elman类型

以上简单的RNN在理论上是可以编码长周期的依赖信息的，但实际训练时缺难以被学到。梯度爆炸和梯度消失问题对于简单RNN来说非常常见。

于是出现了其各种改进版：LSTM、GRU、双向RNN、使用RNN的seq2seq

传统的seq2seq模型解码时使用的是当前隐状态和最后一个时间步的输出。最后一个时间步中并不能包含整个序列的信息。并且RNN不能将整个序列的信息编码到一个定长的隐状态向量中。

![1666428270273](论文精读.assets/1666428270273.png)

RNN大类的神经网络模型在基于神经网络的对话系统由于其编码序列信息的强大能力而充当着重要角色。

任务导向的对话系统将RNN作为**对话上下文**、**对话状态**、**知识库记录**、**领域标签**的编码器

开放域则将RNN应用于**对话历史**的编码器，其中包括基于检索系统的对话历史和候选回复。

在基于知识的系统中，RNN充当外**部知识源**的编码器(比如background、persona、topic, etc.)

RNN作为对话系统的解码器时通常将语句序列的隐状态使用贪婪搜索或者束搜索来进行解码。这种解码机制导致了一些比如万能回复的问题。

[RNN结合其他模型去训练对话embeddings](file:///D:/A/NLP/papers/NLG/A Contrastive Framework for Neural Text Generation.pdf).这些词嵌入模型在对话任务上进行训练，表现了更多的对话特性。在一些对话任务上其效果比未经在具体任务上微调的SOTA上下文表达模型(e.g., BERT, ELMo, and GPT)要好。

**HRED**(Hierarchical Recurrent Encoder-Decoder)

其设计的目的是感知历史查询

之后原作者又在解码器中引入隐变量使得解码步骤变成：1）采样隐变量 2）根据条件生成回复

最近很多工作使用了基于HRED的架构去捕捉多层级的对话特征。有认为标准的HRED将所有对话历史不加区分地进行处理。受Transformer架构影响，Vaswani等人提出ReCoSa，a self-attention-based hierarchical model.Shen等人提出了分三个层级的模型，包括1）篇章级 2）句对级3）句子级，分别捕捉全局知识、句对关键信息、内容信息。Chauhan等人使用HRED和VGG-19组合成为多模态MHRED，HRED编码层级对话信息，同时VGG-19提取对应轮次的所有视觉信息，并添加了一个位置感知的attention。Mehri通过四个子任务学习到了对话的上下文信息，其中的三个子任务(下句生成、掩码句子检索、不一致性识别)将HRED作为上下文信息的编码器。

Cao使用HRED编码患者和医师的对话历史以及客户端MI行为编码，并且预测未来的编码。

Qiu等人使用基于LSTM的VHRED去解决了在无监督数据中去归纳two-agent和multi-agent对话结构问题。除此之外他们在two-agent对话和一个非映射依赖树结构的multi-agent使用了Conditional Random Field model

**Transformer-based pretrain models for dialogue systems**

zhao等人使用一个综合性的数据集构建了一个基于知识的对话系统，该系统使用BERT进行知识的检索，使用GPT-2基于对话上下文和检索到的知识进行对话生成。

**Pointer Net and CopyNet**

在一些NLP任务中，比如对话系统和问题回答，智能体有时需要直接引用用户的信息，Pointer Net就解决了直接从用户输入中复制词元的问题

其对seq2seq模型中最后解码时

###### CopyNet

其提出是为了将copy机制融合进传统的seq2seq模型，由模型决定每个解码阶段是去从资源中复制还是生成资源中没有的新的词元。copynet的编码器与传统的seq2seq一样，解码器在预测时结合了生成模型和copy模型

![1667818723213](论文精读.assets/1667818723213.png)

>Where t is the time step. st is the decoder hidden state and yt is the predicted token. ct and M represent weighted sum of encoder hidden states and encoder hidden states respectively. g and c are generate-mode and copy-mode respectively.

copy机制适用于含有术语或者外部知识的对话，在基于知识的和任务导向的对话系统中广泛应用。

对于基于知识的系统而言，外部文件或者对话就是copy的源。

许多对话状态追踪任务使用a copy component生成槽和槽值

Pointer networks and CopyNet同样用于对话相关的任务，Yu 使用了一个pointer net用来在线对话解纠缠。指针模块指向当前消息所回复的祖先消息，之后使用分类器预测两条消息是否属于同一个线程。 在对话分析任务中，采用Pointer net作为主干分析模型来构建篇章树 

**Deep Reinforcement Learning Models and Generative Adversarial Networks**

近年来，强化学习在许多复杂问题上已经胜过了人类，比如大规模游戏、对话、车辆驾驶。另一个惊艳的技术是GAN，其在生成任务中表现出了惊人的能力，由GAN生成的一些如文章、绘画，甚至是视频已经能够以假乱真。

强化学习目的是训练智能体使其与具体环境交互时做出正确的决策，其是机器学习三大基础分支之一。

<img src="论文精读.assets/1667822621207.png" alt="1667822621207" style="zoom:50%;" />

强化学习的框架如上图，该框架是一个马尔可夫决策过程Markov Decision Process (MDP)，可以被描述为一个五元组M = （S, A,P, R, γ），其中S是一个无限的环境状态空间；A是智能体根据给定的环境状态s选定的动作集合；P是MDP中的转移概率矩阵，代表着智能体做出动作之后环境空间转移的概率；R是智能体在状态s下做出动作后从环境中获得的奖励；γ是折扣系数。该框架的运行遵循以下两步的循环：1）智能体首先观察当前的环境状态$s_t$，并且基于其策略选择一个动作；之后根据转移概率矩阵P，环境转台转移到$s_{t+1}$,同时产出一个回报$γ_t$

由于对话系统智能体-环境交互的本质，强化学习可用于解决该领域的很多问题. 对话系统通常由一个智能体即对话机器人,和环境,即用户或模拟用户

GAN网的架构如下:

<img src="论文精读.assets/1667823924729.png" alt="1667823924729" style="zoom:67%;" />

其包含了一个生成器和一个鉴别器, 训练过程可以被看作是他们之间的对抗:生成器尽力生成数据分布去蒙混过鉴别器,然而鉴别器试图去区分真实数据和生成的假数据.

在训练过程中,生成器将噪声作为输入,鉴别器则将真和假数据作为输入,并且以二进制注释作为标签

GAN网是一个特殊的actor-critic,其actor生成时具有盲目性,并且整个过程是一个不依赖状态的MDP

**RL for task-oriented dialogue systems**

RL被应用于对话管理中的对话状态追踪和策略学习,尤其在对话策略学习中近期的工作都是结合RL.这是基于策略学习的特点:模型基于DST模块去预测对话action,这就完美贴合了智能体在RL中的位置.

**RL for open-domain dialogue systems**

由于直接生成对话需要很大的动作空间,因此许多开放域对话系统使用RL来选择回复,而不是生成回复.基于检索的系统有着有限的动作集合,故适用于RL.

然而检索系统的回复并不能应付所有用户的消息,并且可能给出无关的回复.

Zhu等人选择结合检索和生成的方法来设计系统,首先检索一个长度为n的最优候选回复集合,然后再基于检索的结果和用户所发送的信息去生成.相较而言,Serban等人首先使用不同的对话模型生成并检索候选回复,然后在一个使用在线RL的打分模型去从生成和检索到的回复中选择回复

由于从零开始使用RL训练一个生成对话智能体时很困难的,因此首先将智能体进行监督学习去进行热启动是一个不错的选择.

**RL for knowledge grounded dialogue systems**

一些系统使用RL去选择一些外部信息,如角色信息 文档信息和知识图等,然后根据这些信息去生成回复

在强化学习框架中,智能体每一步基于当前节点和状态选择一条边,然后将知识结合进生成回复的过程中

**RL for dialogue related tasks**



**GAN for dialogue systems**

对话系统中GAN网的应用分为了两个流派, 第一种将GAN框架应用于增强回复生成.鉴别器区分人类的和生成的回复,GAN的生成器则用来生成更高质量的回复.另一个流派则是用GAN作为一个对话系统的评价工具.将生成器和鉴别器作为一个整体架构训练好后, 鉴别器被单独用来去给对话智能体进行打分,其打分结果与人工评分的相关性相比于BLEU, METEOR, ROUGE-L, etc. 都高

**2.8 Knowledge Graph Augmented Neural Networks**

使用带标注数据进行的有监督训练试图学到数据集中的知识分布.  然而一个数据集是相对稀疏的,因此要学到一个可靠的知识分布,需要大量的有注释的数据

###### Knowledge Graph (KG)

是一个由实体和它们之间关系组成的有结构的知识源. 换句话说, KG就是将知识呈现在图结构中.具体KG例子:

![1667893658660](论文精读.assets/1667893658660.png)

KG被存储在以来源描述为架构的三元组中, 其中Albert Einstein, University of Zurich和他们的关系可以表述为(Albert Einstein, Graduate From, University of Zurich)

知识图谱增强的神经网路首先将实体和他们的关系在低维空间中进行表示, 之后使用神经模型去检索图谱中的相关事实. 知识图谱表示学习可以被大致分为两类:1)基于结构的表示 2)语义丰富化的表示

1)使用多维向量去表示实体和关系.2)将语义信息结合进实体和关系的表示中

神经检索模型同样有两个主要方向:1)基于距离配对的模型2)语义配对模型

1)关注映射的实体间的距离 2)计算实体和关系与检索事实间的语义相似度

**Knowledge graph augmented dialogue systems**

基于知识的对话系统可以从KG中的结构化知识大大获益, KG中的事实广泛地相互关联. 通过KG去推理是一个将常识知识结合到回复的生成中的一个理想方法.

##### ***3 Task-oriented Dialogue Systems***

任务导向的对话系统包括**1)模块化的**和**2)端到端的**系统

任务导向系统解决的是具体领域的具体问题,比如预定电影票, 预定餐馆等. 

**模块化系统**

任务导向系统需要更为严格的回复限制, 因为其目的是精确解决用户信息. 因此模块化方法的提出是为了以更为可控的方式去生成回复. 基于模块化的系统架构如下:

![1667905738951](论文精读.assets/1667905738951.png)

###### Natural Language Understanding (NLU)

该模块负责将用户的原始消息转换为**语义槽**, 同时给出**领域分类信息**和**用户意图**. 而最近的一些工作选择越过该过程, 直接使用原始用户消息作为下个模块的输入(上图中有表示出). 这样的设计是为了<u>减少模块间传播的错误, 并且缓解原始错误的影响?</u>

###### Dialogue State Tracking (DST)

该模块基于当前的输入和对话历史迭代式地矫正对话状态. 对话状态包括相关的**用户动作**和**槽值对**.

###### Dialogue Policy Learning

基于由DST矫正了的对话状态,该模块决定对话智能体的下一个动作.

###### Natural Language Generation (NLG)

该模块负责将选中的对话动作转变为语言表达, 即通常为最后的回复.

其中DST和DPL共同组成Dialogue Manager(DM), DM即为任务导向对话系统的中心控制器. 通常, 任务导向系统也与一个外部知识库(external Knowledge Base)进行交互, 去检索关于目标任务的重要信息. 比如在预定电影票的任务中, 在理解用户的需求后, 智能体就可以去电影知识库在相关限制下去搜具体的电影.

**端到端系统**

模块化对话系统存在两个大的弊端:

1) 流水线中的有些模块有时候是不可微的, 这就导致末端的损失不能传达至每个模块. 真实的对话系统训练, 通常唯一的信号就是用户的回复, 而其它的像是对话状态, 对话动作是很少的.

2) 虽然各模块联接起来使得整个对话系统有出色的表现, 但单独某个模块的提升可能并不会使整个系统的对话精准度得到提升. 这会导致其他的模块进行额外的训练, 该训练是费时费力的. 除此之外, 由于流水线中的一些手工定义的特征比如, 对话状态, 使得模块化系统很难迁移到另一个领域, 即预定义的一些东西需要进行修改.

任务导向的对话系统实现端到端的训练有两个主要方法:

1)使得流水线中每个模块都可微, 之后整个流水线就可以被视为一个大的可微的系统, 从而其中的参数可以由反向传播来优化.

2)仅使用一个端到端的模块去进行知识检索和回复生成, 这通常是一个**多任务学习**的神经模型

##### *4 Open-Domain Dialogue Systems*

开放域对话系统是没有具体任务和领域限制的情况下与用户进行闲聊, 通常是数据驱动的.开放域对话系统通常分为三大类:1) 生成式系统 2) 基于检索的系统 3) 集成系统

1) 使用seq2seq的模型, 为用户信息和对话历史配对到一个可能在训练数据集中没有出现过的回复.

2) 则想去找到在确定的回复集合中已经存在的回复

###### some research challenges and hot topics in open-domain dialogue systems

**Context Awareness**

对话上下文包括用户消息和系统消息，由于该信息决定了对话的主题和用户意图，因此是对话智能体产生回复的重要信息来源。具有上下文感知能力的对话智能体在生成回复时应能够结合当前用户消息和上下文去给出回复。先前的基于深度学习的系统将所有对话历史的单词表示直接累加，或者以一个固定大小的窗口去聚焦近期的上下文。Serban等人提出了Hierarchical Recurrent Encoder-Decoder (HRED)，该工作为上下文感知领域开创性的工作。



不论是生成式的还是检索式的对话系统，都对对话上下文建模具有很高的依赖性。shen等人提出了Conversational Semantic Relationship RNN (CSRR)从三个级别去建模对话的上下文：1）语句级别 2）句对级别 3）篇章级别 分别去捕捉内容信息、用户主题、全局主题。zhang指出分层的encoder-decoder在decoder与对话上下文交互时没有很好地强调其中具体的部分。

Feng等人的工作中，不仅利用了对话历史，也用了未来的对话。考虑到在现实的推理中，对话智能体不能明确地知道未来的信息，他们首先使用过去和未来的上下文训练了一个基于场景的模型，然后用了一个仿真框架将场景知识迁移给目标网络。

**Response Coherence**

回复相关性是指对话能够保持逻辑和一致性

**Response Diversity**

**Speaker Consistency and Personality-based Response**

**Empathetic Response**

**Controllable Generation**

**Conversation Topic**

⑤相关工作

⑥模型

⑦实验

⑧评论

---

### GAN

①标题+作者

②摘要

③结论

④导言

深度学习是用来发现丰富的有层次的模型来表示AI中各种应用的各种数据的概率分布表示。深度学习目前在辨别模型上做的比较好，但是在生成模型上还是比较差 ，这是由于在最大化似然函数时需要对其概率分布进行很多近似，这个近似带来了很大的计算困难。GAN则选择不去近似这个似然函数，而是去学习一个模型去近似这个分布

⑤相关工作

⑥模型

⑦实验

⑧评论

---

### UniDS

①标题+作者

A Unified Dialogue System for Chit-Chat and Task-oriented Dialogues

中国科技大学和华为诺亚方舟实验室联合Huawei Noah’s Ark Lab

②摘要

③结论

④导言

⑤相关工作

⑥模型

第t轮的对话的组成成分包括：用户输入$U_t$，对话状态$B_t$,数据库搜索结果$D_t$，系统动作$A_t$，以及回复$R_t$，其中的每个成分都由来自定长词典的词元组成。对于每一轮t，给定$$C_t$$作为对话上下文信息$C_t=[U_0,B_0,D_0,A_0,R_0,……,R_{t-1},U_t]$

首先会根据上下文信息$C_t$去生成对话状态即用户意图的概率分布
$$
B_t = UniDS(C_t) ,
\tag{1}
$$


然后根据对话状态去搜索数据库，获取到搜索结果$D_t$。Afterwards，UniDS生成系统动作$A_t$，该动作由扩充后的上下文信息(新加入了$B_t$、$D_t$)所决定
$$
A_t = UniDS(C_t ⊕ [B_t,D_t])
\tag{2}
$$
最终生成回复
$$
R_t = UniDS(C_t ⊕ [B_t,D_t, A_t])
\tag{3}
$$
UniDS架构如下：

![1668667790524](论文精读.assets/1668667790524.png)

为了使得闲聊和任务导向对话数据能同时进行有效的训练，本文设计了一个统一的数据规范：

![1668669582727](论文精读.assets/1668669582727.png)

有了该标准后，第t轮的数据的输入不论是TOD还是ODD都得到了统一的表示：
$$
X_t = [C_t, B_t,D_t, A_t, R_t]
\tag{4}
$$
UniDS的训练目标就是通过自回归的方式最大化$X_t$中所有tokens的联合概率
$$
l=\sum_{i=1}^N-logP（x_i|x_{<i}）
\tag{5}
$$





$$
l_w=\sum_{i=1}^N-w_ilogP（x_i|x_{<i}）
\tag{6}
$$




⑦实验

⑧评论

---

### ConvLab-2

①标题+作者

An Open-Source Toolkit for **Building**, **Evaluating**, and **Diagnosing** Dialogue Systems

②摘要

ConvLab2是用来帮助研究者使用当前的SOTA模型来构建任务导向对话系统的工具，可以实现端到端的测评，并且能够诊断系统的缺点。提供了一个分析工具和一个交互工具来诊断对话系统。分析工具可以通过模拟对话后展示大量的数据统计，和常规错误的总结，有助于错误分析和系统的提升。交互工具提供了用户界面，开发者可以通过与集成的对话系统进行交互，并且修改每个系统的构件。

③结论

④导言

⑤相关工作

⑥模型

⑦实验

⑧评论



---

### UBAR

①标题+作者

Towards Fully **End-to-End** Task-Oriented Dialog System with **GPT-2**

②摘要

UBAR是任务导向的会话级别的对话模型，是从单向语言模型GPT-2在成序列的整个会话(每轮都包括用户语句、会话状态、数据库查询结果、系统动作，系统回复)微调得来的。UBAR是在更为实际的配置下进行评估的，其对话上下文可以访问到用户的输入，以及所有生成的内容，such as belief states, system acts, and system responses。在MultiWOZ数据集上的结果显示UBAR达到了SOTA水平，提高了回复的综合评分4.7个点，策略学习下3.5，端到端情况下9.4。



③结论

④导言

⑤相关工作

⑥模型

⑦实验

⑧评论

